{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd73316e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kaggle Playground - Loan Payback Prediction (v12 - Optuna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f550e48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240edf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for Preprocessing and Modeling\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Machine Learning Models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "\n",
    "# Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76be492",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90ba46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "TRAIN_PATH = \"/kaggle/input/playground-series-s5e11/train.csv\"\n",
    "TEST_PATH = \"/kaggle/input/playground-series-s5e11/test.csv\"\n",
    "SUBMISSION_PATH = \"/kaggle/input/playground-series-s5e11/sample_submission.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sample_submission = pd.read_csv(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f886914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1be524",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_fe_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def complete_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Comprehensive feature engineering pipeline for loan prediction\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. FINANCIAL RATIOS \n",
    "    df['loan_to_income_ratio'] = df['loan_amount'] / (df['annual_income'] + 1)\n",
    "    df['monthly_income'] = df['annual_income'] / 12\n",
    "    # Simplified approximation from source\n",
    "    df['monthly_payment_estimate'] = (df['loan_amount'] * df['interest_rate']) / 100  \n",
    "    df['payment_to_income_ratio'] = df['monthly_payment_estimate'] / (df['monthly_income'] + 1)\n",
    "    df['current_debt_amount'] = df['debt_to_income_ratio'] * df['annual_income']\n",
    "    df['total_debt_with_loan'] = df['current_debt_amount'] + df['loan_amount']\n",
    "    df['new_debt_to_income'] = df['total_debt_with_loan'] / (df['annual_income'] + 1)\n",
    "    df['debt_increase_ratio'] = df['new_debt_to_income'] / (df['debt_to_income_ratio'] + 1e-6)\n",
    "    df['disposable_income'] = df['annual_income'] - df['current_debt_amount']\n",
    "    df['disposable_income_ratio'] = df['disposable_income'] / (df['annual_income'] + 1)\n",
    "    df['loan_to_disposable_income'] = df['loan_amount'] / (df['disposable_income'] + 1)\n",
    "    df['monthly_disposable_income'] = df['disposable_income'] / 12\n",
    "    df['payment_to_disposable_ratio'] = df['monthly_payment_estimate'] / (df['monthly_disposable_income'] + 1)\n",
    "    df['annual_payment_burden'] = df['monthly_payment_estimate'] * 12\n",
    "    df['payment_burden_ratio'] = df['annual_payment_burden'] / (df['annual_income'] + 1)\n",
    "\n",
    "    # 2. CREDIT SCORE FEATURES \n",
    "    df['credit_score_normalized'] = df['credit_score'] / 850\n",
    "    df['credit_risk_score'] = 1 - df['credit_score_normalized']\n",
    "    df['credit_score_squared'] = df['credit_score'] ** 2\n",
    "    df['credit_score_log'] = np.log1p(df['credit_score'])\n",
    "    df['credit_category'] = pd.cut(df['credit_score'], bins=[0, 580, 670, 740, 800, 850],\n",
    "                                 labels=['poor', 'fair', 'good', 'very_good', 'excellent'])\n",
    "    df['credit_income_interaction'] = df['credit_score'] * df['annual_income']\n",
    "    df['credit_times_dti'] = df['credit_score'] * df['debt_to_income_ratio']\n",
    "    df['credit_loan_interaction'] = df['credit_score'] * df['loan_amount']\n",
    "\n",
    "    # 3. INTEREST RATE FEATURES \n",
    "    df['high_interest_flag'] = (df['interest_rate'] > df['interest_rate'].median()).astype(int)\n",
    "    df['very_high_interest'] = (df['interest_rate'] > df['interest_rate'].quantile(0.75)).astype(int)\n",
    "    df['low_interest_flag'] = (df['interest_rate'] < df['interest_rate'].quantile(0.25)).astype(int)\n",
    "    df['total_interest_cost'] = df['loan_amount'] * df['interest_rate'] / 100\n",
    "    df['interest_burden'] = df['total_interest_cost'] / (df['annual_income'] + 1)\n",
    "    df['interest_credit_mismatch'] = df['interest_rate'] * (1 - df['credit_score_normalized'])\n",
    "    df['interest_credit_ratio'] = df['interest_rate'] / (df['credit_score'] / 100)\n",
    "    df['interest_rate_squared'] = df['interest_rate'] ** 2\n",
    "\n",
    "    # 4. RISK SCORES \n",
    "    df['risk_score_v1'] = (df['debt_to_income_ratio'] * 0.25 + df['loan_to_income_ratio'] * 0.25 +\n",
    "                            df['credit_risk_score'] * 0.30 + (df['interest_rate'] / 100) * 0.20)\n",
    "    df['risk_score_v2'] = (df['payment_to_income_ratio'] * 0.40 + df['new_debt_to_income'] * 0.35 +\n",
    "                            df['interest_burden'] * 0.25)\n",
    "    df['affordability_score'] = (df['credit_score_normalized'] * 0.40 +\n",
    "                                (1 - df['debt_to_income_ratio']) * 0.30 +\n",
    "                                df['disposable_income_ratio'] * 0.30)\n",
    "    df['financial_health_score'] = df['affordability_score'] * 0.60 - df['risk_score_v1'] * 0.40\n",
    "\n",
    "    # 5. LOAN AMOUNT FEATURES \n",
    "    df['loan_size'] = pd.cut(df['loan_amount'], bins=[0, 10000, 20000, 30000, np.inf],\n",
    "                             labels=['small', 'medium', 'large', 'very_large'])\n",
    "    df['loan_amount_squared'] = df['loan_amount'] ** 2\n",
    "    df['loan_amount_log'] = np.log1p(df['loan_amount'])\n",
    "    df['annual_income_log'] = np.log1p(df['annual_income'])\n",
    "    df['loan_amount_sqrt'] = np.sqrt(df['loan_amount'])\n",
    "\n",
    "    # 6. BINNING FEATURES\n",
    "    df['income_decile'] = pd.qcut(df['annual_income'], q=10, labels=False, duplicates='drop')\n",
    "    df['credit_decile'] = pd.qcut(df['credit_score'], q=10, labels=False, duplicates='drop')\n",
    "    df['loan_decile'] = pd.qcut(df['loan_amount'], q=10, labels=False, duplicates='drop')\n",
    "    df['dti_decile'] = pd.qcut(df['debt_to_income_ratio'], q=10, labels=False, duplicates='drop')\n",
    "    df['interest_decile'] = pd.qcut(df['interest_rate'], q=10, labels=False, duplicates='drop')\n",
    "\n",
    "    # 7. INTERACTION FEATURES \n",
    "    df['income_x_credit'] = df['annual_income'] * df['credit_score']\n",
    "    df['dti_x_interest'] = df['debt_to_income_ratio'] * df['interest_rate']\n",
    "    df['loan_x_interest'] = df['loan_amount'] * df['interest_rate']\n",
    "    df['income_x_dti'] = df['annual_income'] * df['debt_to_income_ratio']\n",
    "    df['income_credit_loan'] = (df['annual_income'] * df['credit_score']) / (df['loan_amount'] + 1)\n",
    "    df['dti_interest_credit'] = (df['debt_to_income_ratio'] * df['interest_rate']) / (df['credit_score_normalized'] + 1e-6)\n",
    "\n",
    "    # 8. GRADE FEATURES \n",
    "    df['grade'] = df['grade_subgrade'].str[0]\n",
    "    df['subgrade_num'] = pd.to_numeric(df['grade_subgrade'].str[1:], errors='coerce')\n",
    "    grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    df['grade_numeric'] = df['grade'].map(grade_map)\n",
    "    df['full_grade_score'] = df['grade_numeric'] * 10 + df['subgrade_num']\n",
    "    df['grade_credit_ratio'] = df['full_grade_score'] / (df['credit_score'] / 100)\n",
    "\n",
    "    # 9. STATISTICAL AGGREGATIONS \n",
    "    financial_metrics = ['debt_to_income_ratio', 'loan_to_income_ratio', 'payment_to_income_ratio']\n",
    "    df['mean_financial_metrics'] = df[financial_metrics].mean(axis=1)\n",
    "    df['max_financial_burden'] = df[financial_metrics].max(axis=1)\n",
    "    df['min_financial_burden'] = df[financial_metrics].min(axis=1)\n",
    "    df['std_financial_metrics'] = df[financial_metrics].std(axis=1)\n",
    "\n",
    "    # 10. CATEGORICAL COMBINATIONS \n",
    "    df['gender_marital'] = df['gender'] + '_' + df['marital_status']\n",
    "    df['education_employment'] = df['education_level'] + '_' + df['employment_status']\n",
    "    df['gender_education'] = df['gender'] + '_' + df['education_level']\n",
    "    df['marital_employment'] = df['marital_status'] + '_' + df['employment_status']\n",
    "    df['purpose_grade'] = df['loan_purpose'] + '_' + df['grade']\n",
    "    df['employment_purpose'] = df['employment_status'] + '_' + df['loan_purpose']\n",
    "\n",
    "    # 11. ANOMALY FLAGS \n",
    "    df['extreme_dti'] = (df['debt_to_income_ratio'] > df['debt_to_income_ratio'].quantile(0.95)).astype(int)\n",
    "    df['low_income'] = (df['annual_income'] < df['annual_income'].quantile(0.25)).astype(int)\n",
    "    df['large_loan'] = (df['loan_amount'] > df['loan_amount'].quantile(0.75)).astype(int)\n",
    "    df['risky_combo_1'] = ((df['debt_to_income_ratio'] > 0.4) & (df['credit_score'] < 600)).astype(int)\n",
    "    df['risky_combo_2'] = ((df['loan_to_income_ratio'] > 0.5) & (df['interest_rate'] > 15)).astype(int)\n",
    "    df['safe_combo'] = ((df['credit_score'] > 750) & (df['debt_to_income_ratio'] < 0.1)).astype(int)\n",
    "    df['high_risk_all'] = (df['extreme_dti'] & df['risky_combo_1']).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_fe_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Starting Feature Engineering...\")\n",
    "train_fe = complete_feature_engineering(train)\n",
    "test_fe = complete_feature_engineering(test)\n",
    "print(\"Feature Engineering Complete.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12047b07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba0baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET = 'loan_paid_back'\n",
    "y = train[TARGET]\n",
    "\n",
    "# Handle NaNs created during FE (e.g., subgrade_num)\n",
    "train_fe['subgrade_num'] = train_fe['subgrade_num'].fillna(0)\n",
    "test_fe['subgrade_num'] = test_fe['subgrade_num'].fillna(0)\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "cat_features = train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_features = train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Remove target and ID from feature lists\n",
    "num_features = [col for col in num_features if col not in [TARGET, 'id']]\n",
    "original_cat_features = train.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_features = list(set(cat_features + original_cat_features) - {'grade_subgrade'}) # 'grade_subgrade' is removed, 'grade' is added\n",
    "\n",
    "print(f\"Categorical Features: {cat_features}\")\n",
    "print(f\"Numerical Features: {len(num_features)}\")\n",
    "\n",
    "# Ordinal Encoding for all categorical features\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "train_fe[cat_features] = encoder.fit_transform(train_fe[cat_features])\n",
    "test_fe[cat_features] = encoder.transform(test_fe[cat_features])\n",
    "\n",
    "# Standard Scaling for numerical features\n",
    "scaler = StandardScaler()\n",
    "train_fe[num_features] = scaler.fit_transform(train_fe[num_features])\n",
    "test_fe[num_features] = scaler.transform(test_fe[num_features])\n",
    "\n",
    "# Combine features for modeling\n",
    "features = num_features + cat_features\n",
    "X = train_fe[features]\n",
    "X_test = test_fe[features]\n",
    "\n",
    "print(f\"Final feature count: {len(features)}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optuna_markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_setup",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a single validation split for quick Optuna tuning\n",
    "# We will use a full CV later with the best params\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_OPTUNA_TRIALS = 75\n",
    "\n",
    "# Get feature indices for CatBoost\n",
    "cat_features_indices = [X.columns.get_loc(c) for c in cat_features if c in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_lgbm",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_lgbm(trial):\n",
    "    param = {\n",
    "        'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0,\n",
    "        'objective': 'binary', 'metric': 'auc',\n",
    "        'boosting_type': 'gbdt', \n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "        'random_state': RANDOM_STATE, 'verbosity': -1, 'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**param)\n",
    "    model.fit(X_train_tune, y_train_tune, \n",
    "              eval_set=[(X_val_tune, y_val_tune)], \n",
    "              eval_metric='auc', \n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val_tune)[:, 1]\n",
    "    auc = roc_auc_score(y_val_tune, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "print(\"Tuning LGBMClassifier...\")\n",
    "study_lgbm = optuna.create_study(direction='maximize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_lgb = study_lgbm.best_params\n",
    "print(f\"Best LGBM AUC: {study_lgbm.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_xgb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'gpu_id': 0,\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'auc',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.7, 0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5, 0.7, 0.9, 1.0]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n",
    "        'random_state': RANDOM_STATE, 'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train_tune, y_train_tune, \n",
    "              eval_set=[(X_val_tune, y_val_tune)], \n",
    "              early_stopping_rounds=100, \n",
    "              verbose=False)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val_tune)[:, 1]\n",
    "    auc = roc_auc_score(y_val_tune, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "print(\"Tuning XGBClassifier...\")\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(f\"Best XGB AUC: {study_xgb.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_cat",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_cat(trial):\n",
    "    param = {\n",
    "        'task_type': 'GPU', 'devices': '0',\n",
    "        'loss_function': 'Logloss', 'eval_metric': 'AUC',\n",
    "        'iterations': trial.suggest_int('iterations', 500, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0, log=True),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 128, 254),\n",
    "        'random_seed': RANDOM_STATE, 'logging_level': 'Silent',\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "\n",
    "    train_pool = Pool(X_train_tune, y_train_tune, cat_features=cat_features_indices)\n",
    "    val_pool = Pool(X_val_tune, y_val_tune, cat_features=cat_features_indices)\n",
    "    \n",
    "    model = CatBoostClassifier(**param)\n",
    "    model.fit(train_pool, eval_set=val_pool, verbose=0)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(val_pool)[:, 1]\n",
    "    auc = roc_auc_score(y_val_tune, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "print(\"Tuning CatBoostClassifier...\")\n",
    "study_cat = optuna.create_study(direction='maximize')\n",
    "study_cat.optimize(objective_cat, n_trials=N_OPTUNA_TRIALS)\n",
    "best_params_cat = study_cat.best_params\n",
    "print(f\"Best CAT AUC: {study_cat.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6264797",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Model Training (Cross-Validation with Tuned Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv_setup_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# OOF and Test Predictions Arrays\n",
    "lgb_oof = np.zeros(len(X))\n",
    "xgb_oof = np.zeros(len(X))\n",
    "cat_oof = np.zeros(len(X))\n",
    "\n",
    "lgb_test = np.zeros(len(X_test))\n",
    "xgb_test = np.zeros(len(X_test))\n",
    "cat_test = np.zeros(len(X_test))\n",
    "\n",
    "# Score Lists\n",
    "lgb_scores = []\n",
    "xgb_scores = []\n",
    "cat_scores = []\n",
    "\n",
    "# --- Add final GPU/Fixed params to the tuned params ---\n",
    "best_params_lgb.update({'device': 'gpu', 'gpu_platform_id': 0, 'gpu_device_id': 0, 'objective': 'binary', 'metric': 'auc', 'random_state': RANDOM_STATE, 'verbosity': -1, 'n_jobs': -1})\n",
    "best_params_xgb.update({'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'gpu_id': 0, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'random_state': RANDOM_STATE, 'n_jobs': -1})\n",
    "best_params_cat.update({'task_type': 'GPU', 'devices': '0', 'loss_function': 'Logloss', 'eval_metric': 'AUC', 'random_seed': RANDOM_STATE, 'logging_level': 'Silent', 'early_stopping_rounds': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cv_loop_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    # --- 1. LightGBM ---\n",
    "    print(\"Training LGBM...\")\n",
    "    lgb = LGBMClassifier(**best_params_lgb)\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_val, y_val)], \n",
    "            eval_metric='auc', \n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "    lgb_oof[val_idx] = lgb.predict_proba(X_val)[:, 1]\n",
    "    lgb_test += lgb.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "    lgb_scores.append(roc_auc_score(y_val, lgb_oof[val_idx]))\n",
    "\n",
    "    # --- 2. XGBoost ---\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb = XGBClassifier(**best_params_xgb)\n",
    "    xgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_val, y_val)], \n",
    "            early_stopping_rounds=100, \n",
    "            verbose=False)\n",
    "    \n",
    "    xgb_oof[val_idx] = xgb.predict_proba(X_val)[:, 1]\n",
    "    xgb_test += xgb.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "    xgb_scores.append(roc_auc_score(y_val, xgb_oof[val_idx]))\n",
    "\n",
    "    # --- 3. CatBoost ---\n",
    "    print(\"Training CatBoost...\")\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features_indices)\n",
    "    val_pool = Pool(X_val, y_val, cat_features=cat_features_indices)\n",
    "    \n",
    "    cat = CatBoostClassifier(**best_params_cat)\n",
    "    cat.fit(train_pool, eval_set=val_pool, verbose=0)\n",
    "    \n",
    "    cat_oof[val_idx] = cat.predict_proba(val_pool)[:, 1]\n",
    "    cat_test += cat.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "    cat_scores.append(roc_auc_score(y_val, cat_oof[val_idx]))\n",
    "    \n",
    "    print(f\"Fold {fold+1} Scores: LGB={lgb_scores[-1]:.6f}, XGB={xgb_scores[-1]:.6f}, CAT={cat_scores[-1]:.6f}\")\n",
    "    del lgb, xgb, cat, train_pool, val_pool\n",
    "    gc.collect()\n",
    "\n",
    "# --- Final OOF Scores ---\n",
    "lgb_score = roc_auc_score(y, lgb_oof)\n",
    "xgb_score = roc_auc_score(y, xgb_oof)\n",
    "cat_score = roc_auc_score(y, cat_oof)\n",
    "\n",
    "print(f\"\\n--- Overall OOF Scores ---\")\n",
    "print(f\"LightGBM OOF AUC: {lgb_score:.6f}\")\n",
    "print(f\"XGBoost OOF AUC:  {xgb_score:.6f}\")\n",
    "print(f\"CatBoost OOF AUC: {cat_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa16b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble1_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Comparison \n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'OOF AUC': [lgb_score, xgb_score, cat_score]\n",
    "}).sort_values('OOF AUC', ascending=False)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble2_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Ensemble \n",
    "print(\"\\nCREATING ENSEMBLE\")\n",
    "\n",
    "# 1. Simple Average\n",
    "simple_oof = (lgb_oof + xgb_oof + cat_oof) / 3\n",
    "simple_test = (lgb_test + xgb_test + cat_test) / 3\n",
    "simple_score = roc_auc_score(y, simple_oof)\n",
    "\n",
    "# 2. Weighted Average \n",
    "total_auc = lgb_score + xgb_score + cat_score\n",
    "w_lgb = lgb_score / total_auc\n",
    "w_xgb = xgb_score / total_auc\n",
    "w_cat = cat_score / total_auc\n",
    "weighted_oof = (lgb_oof * w_lgb) + (xgb_oof * w_xgb) + (cat_oof * w_cat)\n",
    "weighted_test = (lgb_test * w_lgb) + (xgb_test * w_xgb) + (cat_test * w_cat)\n",
    "weighted_score = roc_auc_score(y, weighted_oof)\n",
    "\n",
    "# 3. Rank Average \n",
    "rank_oof = (rankdata(lgb_oof) + rankdata(xgb_oof) + rankdata(cat_oof)) / (3 * len(lgb_oof))\n",
    "rank_test = (rankdata(lgb_test) + rankdata(xgb_test) + rankdata(cat_test)) / (3 * len(lgb_test))\n",
    "rank_score = roc_auc_score(y, rank_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble3_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble Results\n",
    "ensemble_results = pd.DataFrame({\n",
    "    'Ensemble': ['Simple Average', 'Weighted Average', 'Rank Average'],\n",
    "    'OOF AUC': [simple_score, weighted_score, rank_score]\n",
    "}).sort_values('OOF AUC', ascending=False) \n",
    "\n",
    "print(\"\\nEnsemble Results:\")\n",
    "print(ensemble_results)\n",
    "print(f\"\\nWeights: LGB={w_lgb:.3f}, XGB={w_xgb:.3f}, CAT={w_cat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble4_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose best\n",
    "best_idx = ensemble_results['OOF AUC'].idxmax()\n",
    "best_name = ensemble_results.loc[best_idx, 'Ensemble']\n",
    "best_score = ensemble_results.loc[best_idx, 'OOF AUC']\n",
    "\n",
    "if best_name == 'Simple Average':\n",
    "    final_preds = simple_test\n",
    "elif best_name == 'Weighted Average':\n",
    "    final_preds = weighted_test\n",
    "else:\n",
    "    final_preds = rank_test \n",
    "\n",
    "print(f\"\\nBest Ensemble: {best_name} (AUC: {best_score:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1165f66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8711c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- 8. Submission ---\")\n",
    "submission = pd.DataFrame({'id': test['id'], TARGET: final_preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99da890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(submission[TARGET], bins=50, kde=True)\n",
    "plt.title('Distribution of Final Predictions')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "GPU",
   "datasetSources": [],
   "dockerImageVersionId": 20240,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "kernelSources": [],
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
